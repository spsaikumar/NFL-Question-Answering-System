# -*- coding: utf-8 -*-
"""Llama-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rQFk2cGuICC9kVIFWkoEdvpJScny2lD0
"""

#Installing all necessary libaries
!pip install langchain
!pip install transformers
!pip install huggingface_hub
!pip install "unstructured[all-docs]"
!pip install faiss-gpu
!pip install faiss-cpu
!pip install sentence-transformers
!pip install accelerate
!pip install bitsandbytes

"""IMPORTS"""

# Libraries for document processing, embedding, and question-answering
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQAWithSourcesChain
from huggingface_hub import notebook_login
from transformers import pipeline
from transformers import AutoConfig, AutoModelForCausalLM,AutoTokenizer
from langchain import HuggingFacePipeline
from langchain.text_splitter import CharacterTextSplitter

# General purpose libraries and utilities
import textwrap
import torch
import sys
import os

"""### SETUP"""

# Setting up HuggingFace Hub authentication
os.environ['HuggingFaceHub_API_Token']= 'enter_api_token_here'
notebook_login()

"""DOCUMENT LOADING"""

# Load documents from a given file (e.g., 'file.pdf')
loader = UnstructuredFileLoader('file.pdf')
documents = loader.load()

"""DOCUMENT PROCESSING"""

# Split documents into manageable chunks for better processing.
# This helps in extracting and working with specific passages from the text.
text_splitter=CharacterTextSplitter(separator='\n',
                                    chunk_size=1000,
                                    chunk_overlap=50)
text_chunks=text_splitter.split_documents(documents)

"""EMBEDDING"""

# Generating embeddings using a pre-trained model from HuggingFace.
# The embeddings will represent the semantic content of our text chunks.
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})

"""VECTOR STORE"""

# Storing the embeddings in a FAISS vector store.
# FAISS allows efficient similarity search and retrieval of the chunks.
vectorstore=FAISS.from_documents(text_chunks, embeddings)

"""MODEL SETUP FOR QUESTION ANSWERING"""

# Setting up the tokenizer and model for the LLM
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
                                             device_map='auto',
                                             torch_dtype=torch.float16,
                                             use_auth_token=True,
                                             load_in_8bit=True,
                                              #load_in_4bit=True
                                             )

# Defining the pipeline for text generation using the model and tokenizer.
pipe = pipeline("text-generation",
                model=model,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 1024,
                do_sample=True,
                top_k=10,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id
                )

# Integrating the HuggingFace pipeline with langchain, setting temperature to 0 for deterministic outputs.
llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})

# Establishing the retrieval-based QA chain using our previously created vector store for document retrieval.
chain =  RetrievalQA.from_chain_type(llm=llm, chain_type = "stuff",return_source_documents=True, retriever=vectorstore.as_retriever())

# query = "Which teams played in the NFL Kickoff Game to begin the 2022 season, and what was the result?"
# result=chain({"query": query}, return_only_outputs=True)
# wrapped_text = textwrap.fill(result['result'], width=500)
# wrapped_text

"""QUESTION ANSWERING"""

# Test
# Looping over questions, retrieving answers and printing them.
questions = [
    "Which teams played in the NFL Kickoff Game to begin the 2022 season, and what was the result?",
    "Why was the game between Buffalo and Cincinnati cancelled?",
    "What new name was adopted by the former Washington Redskins for the 2022 NFL season?",
    "State the career trajectory of Tra Blake leading up to his promotion to replace Tony Corrente as a referee in the 2022 NFL season.",
    "Who was hired as the league's first Asian-American NFL official and from which conference was this individual recruited?",
     "In regard to the inclusive hiring training announced at the NFL Fall League Meeting on October 18, who is mandated to participate and under what circumstances must this training be undertaken?"
]

for query in questions:
    result=chain({"query": query}, return_only_outputs=True)
    wrapped_text = textwrap.fill(result['result'], width=500)
    print(wrapped_text)
    print("*"*50)



#Implenting for different values of parameter: temperature for each query to compare response and how paramter will affect the response.
# Implementing a list of queries (questions) and for each query
# Generating  model responses for various temperature values.
# To observe and compare how the temperature parameter affects the response quality and diversity

temperature_values = [0, 0.2, 0.5, 0.8, 1]


for query in questions:
    print(f"Question: {query}")

    for temp in temperature_values:
        llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature': temp})
        chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", return_source_documents=True, retriever=vectorstore.as_retriever())

        result = chain({"query": query}, return_only_outputs=True)
        wrapped_text = textwrap.fill(result['result'], width=500)

        print(f"\nTemperature {temp} Result:")
        print(wrapped_text)
        print("*"*50)

    print("\n\n"+"="*50+"\n\n")

"""## Observation

Factual Accuracy: Extracted answers should be from the provided document and not hallucinated - **Partially Achieved**.

- Model retrieve from the document, some answers, like the one about the Buffalo and Cincinnati game, might be improved. The model's response ("I don't know the answer to your question...") indicates potential improvements in the retrieval or understanding mechanism.

- Due to low computational power, Here opted lower paramters (small model) of model of Llama-2. Using better LLM and Embedding model could get response even more better.

- The "temperature" parameter controls the randomness of the model's outputs. A lower temperature makes the outputs more deterministic and potentially more faithful to the source, while a higher temperature introduces more variation, which can capture nuance but can also introduce inaccuracies.
  - From above output,  a lower temperature (like 0 or 0.2) would be recommended.

### References and Acknowledgements

- https://docs.langchain.com/docs/
- Youtber: Krishnaik
- Youtuber: James Briggs
- https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476

- https://huggingface.co/blog/llama2
- https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-70b-chat-agent.ipynb
"""

